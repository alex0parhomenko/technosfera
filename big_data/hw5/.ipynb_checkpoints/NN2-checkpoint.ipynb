{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "import theano.tensor as th\n",
    "from scipy import misc\n",
    "import copy\n",
    "from numpy.random import uniform\n",
    "from numpy.random import normal\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_classification\n",
    "from math import copysign\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self, layers_list, learning_rate, alpha, activation_functions, cost_func, mode, \\\n",
    "                 cou_iter, early_stop, regularization, reg_param, batch_size):\n",
    "        self.layers_count = len(layers_list)\n",
    "        self.weight_list = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers_list = layers_list\n",
    "        self.alpha = alpha\n",
    "        self.cost_func = cost_func\n",
    "        self.activation_functions = activation_functions\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.cou_iter = cou_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.regularization = regularization\n",
    "        self.reg_param = reg_param\n",
    "        for i in range(1, self.layers_count):\n",
    "            m = np.asarray(normal(0, 0.15, (self.layers_list[i - 1] + 1, self.layers_list[i])))\n",
    "            self.weight_list.append(m)\n",
    "            \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        val = one / (one + np.exp(-x * self.alpha))\n",
    "        for i in range(len(val)):\n",
    "            if (val[i] == 0):\n",
    "                val[i] = 0.00001\n",
    "            elif (val[i] == 1):\n",
    "                val[i] = 0.99999\n",
    "        return one / (one + np.exp(-x * self.alpha))\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        return (one - self.sigmoid(x)) * self.sigmoid(x) * self.alpha\n",
    "    \n",
    "    def x(self, x):\n",
    "        return x\n",
    "    \n",
    "    def der_x(self, x):\n",
    "        return np.ones(len(x))\n",
    "    \n",
    "    def hyp_tg(self, x):\n",
    "        return (np.exp(self.alpha * x) - np.exp(-self.alpha * x)) / (np.exp(self.alpha * x) + np.exp(-self.alpha * x))\n",
    "    \n",
    "    def der_hyp_tg(self, x):\n",
    "        return self.alpha * (1 - (self.hyp_tg(x)) ** 2)\n",
    "    \n",
    "    def logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z.copy()\n",
    "            val = -np.sum(y_true * np.log(y_pred) + (one - y_true) * np.log(one - y_pred))\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return val\n",
    "    \n",
    "    def der_logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z\n",
    "            val = ((one*1.0 - y_true*1.0) / (one - y_pred*1.0) - (y_true*1.0 / y_pred))\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = ((1.0 - y_true) / (1.0 - y_pred) - y_true / y_pred)\n",
    "        return val\n",
    "    \n",
    "    def add_two_lists(self, l1, l2):\n",
    "        for pos in range(len(l1)):\n",
    "            l1[pos] += l2[pos]\n",
    "        return l1\n",
    "    \n",
    "    def div_list(self, l1, m):\n",
    "        for pos in range(len(l1)):\n",
    "            l1[pos] /= m\n",
    "        return l1\n",
    "    \n",
    "    def square_cost(self, y_true, y_pred):\n",
    "        return  0.5*((y_true - y_pred) ** 2.0)\n",
    "    \n",
    "    def der_square_cost(self, y_true, y_pred):\n",
    "        return (y_pred - y_true)\n",
    "    \n",
    "    def add_first_layer(self, x_shape):\n",
    "        self.weight_list.insert(0, np.asarray(normal(0, 0.15, (x_shape, self.layers_list[0]))))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x)\n",
    "        x = np.insert(x, 0, [-1], axis = 1)\n",
    "        ans = []\n",
    "        for s_num, sample in enumerate(x):\n",
    "            v = sample\n",
    "            for num, layer in enumerate(self.weight_list):\n",
    "                v = np.dot(layer.T, v)\n",
    "                if (self.activation_functions[num] == 'sigmoid'):\n",
    "                    v = self.sigmoid(v)\n",
    "                elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                    v = self.hyp_tg(v)\n",
    "                elif (self.activation_functions[num] == 'x'):\n",
    "                    v = self.x(v)\n",
    "                if (num != self.layers_count - 1):\n",
    "                    v = np.insert(v, 0, [-1])\n",
    "            if (self.mode == 'class'):\n",
    "                ans.append(np.argmax(v))\n",
    "            elif (self.mode == 'reg'):\n",
    "                ans.append(v)\n",
    "        return ans\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        y = np.asarray(y)\n",
    "        x = np.insert(np.asarray(x), 0, -1, axis = 1)\n",
    "        self.add_first_layer(x.shape[1])\n",
    "        k = 0\n",
    "        total_err = np.inf\n",
    "        x_size = len(x)\n",
    "        ind_frag = x_size * 0.9\n",
    "        x, y = shuffle(x, y)\n",
    "        test_x = x[ind_frag:]\n",
    "        test_y = y[ind_frag:]\n",
    "        x_new = x[:ind_frag]\n",
    "        y_new = y[:ind_frag]\n",
    "        err_arr = []\n",
    "        result_matrix = []\n",
    "        while (k < self.cou_iter and (len(err_arr) < 10 or err_arr[-1] - err_arr[len(err_arr) - 9] >= self.early_stop)):\n",
    "            print k, total_err\n",
    "            total_err = 0.0\n",
    "            x_new, y_new = shuffle(x_new, y_new)\n",
    "            balance = self.batch_size\n",
    "            for s_num, sample in enumerate(x_new):\n",
    "                now_matrix = []\n",
    "                if (balance == 0):\n",
    "                    #print 'I here!!!'\n",
    "                    #return 0\n",
    "                    balance = self.batch_size\n",
    "                    self.weight_list = self.add_two_lists(self.weight_list, self.div_list(result_matrix, self.batch_size))\n",
    "                    result_matrix = []\n",
    "                neuron_sum = []\n",
    "                neuron_out = []\n",
    "                neuron_out.append(np.asarray(sample, dtype = np.float))\n",
    "                v = sample\n",
    "                for num, layer in enumerate(self.weight_list):\n",
    "                    v = np.dot(layer.T, v)\n",
    "                    neuron_sum.append(v)\n",
    "                    if (self.activation_functions[num] == 'sigmoid'):\n",
    "                        v = self.sigmoid(v)\n",
    "                    elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                        v = self.hyp_tg(v)\n",
    "                    elif(self.activation_functions[num] == 'x'):\n",
    "                        v = self.x(v) \n",
    "                    if (num != self.layers_count - 1):\n",
    "                        v = np.insert(v, 0, [-1])\n",
    "                    neuron_out.append(v)\n",
    "                \n",
    "                if (neuron_out[-1].shape[0] == 1):\n",
    "                    neuron_out[-1] = neuron_out[-1][0]\n",
    "\n",
    "                if (self.cost_func == \"logistic\" and self.activation_functions[-1] == \"sigmoid\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_sigmoid(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == \"logistic\" and self.activation_functions[-1] == \"hyp_tg\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_hyp_tg(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == 'logistic' and self.activation_functions[-1] == \"x\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_x(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == 'square' and self.activation_functions[-1] == \"x\"):\n",
    "                    der_neuron = [(self.der_square_cost(y_new[s_num], neuron_out[-1]) * self.der_x(neuron_sum[-1]))]\n",
    "                \n",
    "                num = 1\n",
    "                for layer in reversed(self.weight_list[1:]):\n",
    "                    if (self.activation_functions[self.layers_count - num - 1] == 'sigmoid'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_sigmoid(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    elif (self.activation_functions[self.layers_count - num - 1] == 'hyp_tg'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_hyp_tg(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    elif (self.activation_functions[self.layers_count - num - 1] == 'x'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_x(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    num += 1\n",
    "\n",
    "                for layer_num in range(self.layers_count):\n",
    "                    #if (self.regularization == 'l1'):\n",
    "                    #    self.weight_list[self.layers_count - layer_num - 1] -= self.learning_rate *( \\\n",
    "                    #    np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)) +\\\n",
    "                    #    self.reg_param * np.sign(self.weight_list[self.layers_count - layer_num - 1]))\n",
    "                        \n",
    "                    #elif (self.regularization == 'l2'):\n",
    "                    #    self.weight_list[self.layers_count - layer_num - 1] -= self.learning_rate *(\\\n",
    "                    #    np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)) +\\\n",
    "                    #    self.reg_param * self.weight_list[self.layers_count - layer_num - 1])\n",
    "                    #if (balance == self.batch_size):\n",
    "                    now_matrix.insert(0, -1.0*self.learning_rate* np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)))\n",
    "                if (len(result_matrix) == 0):\n",
    "                    result_matrix = list(now_matrix)\n",
    "                else:\n",
    "                    result_matrix = self.add_two_lists(result_matrix, now_matrix)\n",
    "                balance -= 1\n",
    "                #print len(result_matrix)\n",
    "                \n",
    "            k += 1\n",
    "            y_pred = self.predict(test_x[:, 1:])\n",
    "            if (self.mode == 'class'):\n",
    "                total_err = accuracy_score(test_y, y_pred)\n",
    "            elif (self.mode == 'reg'):\n",
    "                total_err = mean_squared_error(test_y, y_pred)\n",
    "            err_arr.append(total_err)\n",
    "            #print result_matrix\n",
    "            #return 0\n",
    "            \n",
    "            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.454545454545\n",
      "2 0.454545454545\n",
      "3 0.454545454545\n",
      "4 0.454545454545\n",
      "5 0.454545454545\n",
      "6 0.909090909091\n",
      "7 0.909090909091\n",
      "8 0.909090909091\n",
      "9 0.909090909091\n",
      "10 0.909090909091\n",
      "11 0.454545454545\n",
      "12 0.909090909091\n",
      "13 0.909090909091\n",
      "14 1.0\n",
      "15 0.909090909091\n",
      "16 1.0\n",
      "17 1.0\n",
      "18 0.909090909091\n",
      "19 1.0\n",
      "20 1.0\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 1.0\n",
      "27 1.0\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 1.0\n",
      "31 1.0\n",
      "32 1.0\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 1.0\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 1.0\n",
      "39 1.0\n",
      "40 1.0\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 1.0\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 1.0\n",
      "49 1.0\n",
      "[1, 2, 0, 2, 2, 2, 1, 0, 1, 2, 2, 0, 1, 0, 2, 2, 0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 1, 1, 0, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1] [1 2 0 2 1 2 1 0 1 2 1 0 1 0 2 2 0 1 1 0 2 1 2 0 2 1 1 1 1 0 2 2 1 1 1 2 1\n",
      " 2 2 1]\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "xtest = x[110:]\n",
    "ytest = y[110:]\n",
    "x = x[:110]\n",
    "y = y[:110]\n",
    "\n",
    "nt = network([10, 3], 0.05, 1.0, ['sigmoid', 'sigmoid'], \"logistic\", 'class', 50, early_stop=-1.0, \\\n",
    "             regularization='l2', reg_param = 0.0, batch_size = 5)\n",
    "nt.fit(x, y)\n",
    "ypred = nt.predict(xtest)\n",
    "print ypred, ytest\n",
    "print accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.355555555556\n",
      "2 0.444444444444\n",
      "3 0.444444444444\n",
      "4 0.422222222222\n",
      "5 0.466666666667\n",
      "6 0.466666666667\n",
      "7 0.466666666667\n",
      "8 0.466666666667\n",
      "9 0.466666666667\n",
      "10 0.466666666667\n",
      "11 0.488888888889\n",
      "12 0.488888888889\n",
      "13 0.511111111111\n",
      "14 0.488888888889\n",
      "15 0.488888888889\n",
      "16 0.511111111111\n",
      "17 0.511111111111\n",
      "18 0.511111111111\n",
      "19 0.488888888889\n",
      "20 0.488888888889\n",
      "21 0.511111111111\n",
      "22 0.488888888889\n",
      "23 0.511111111111\n",
      "24 0.511111111111\n",
      "25 0.533333333333\n",
      "26 0.533333333333\n",
      "27 0.533333333333\n",
      "28 0.555555555556\n",
      "29 0.577777777778\n",
      "30 0.6\n",
      "31 0.622222222222\n",
      "32 0.622222222222\n",
      "33 0.622222222222\n",
      "34 0.644444444444\n",
      "35 0.622222222222\n",
      "36 0.622222222222\n",
      "37 0.622222222222\n",
      "38 0.622222222222\n",
      "39 0.622222222222\n",
      "40 0.622222222222\n",
      "41 0.622222222222\n",
      "42 0.6\n",
      "43 0.577777777778\n",
      "44 0.577777777778\n",
      "45 0.6\n",
      "46 0.6\n",
      "47 0.6\n",
      "48 0.6\n",
      "49 0.6\n",
      "50 0.6\n",
      "51 0.6\n",
      "52 0.6\n",
      "53 0.6\n",
      "54 0.6\n",
      "55 0.6\n",
      "56 0.577777777778\n",
      "57 0.577777777778\n",
      "58 0.577777777778\n",
      "59 0.577777777778\n",
      "60 0.577777777778\n",
      "61 0.577777777778\n",
      "62 0.577777777778\n",
      "63 0.577777777778\n",
      "64 0.577777777778\n",
      "65 0.577777777778\n",
      "66 0.577777777778\n",
      "67 0.555555555556\n",
      "68 0.555555555556\n",
      "69 0.555555555556\n",
      "70 0.555555555556\n",
      "71 0.555555555556\n",
      "72 0.555555555556\n",
      "73 0.555555555556\n",
      "74 0.555555555556\n",
      "75 0.555555555556\n",
      "76 0.555555555556\n",
      "77 0.555555555556\n",
      "78 0.555555555556\n",
      "79 0.577777777778\n",
      "0.62\n"
     ]
    }
   ],
   "source": [
    "x, y = make_classification(n_samples=500, n_features=20, n_informative=10, n_classes=4)\n",
    "xtest = x[450:]\n",
    "ytest = y[450:]\n",
    "x = x[:450]\n",
    "y = y[:450]\n",
    "nt = network([20, 4], 0.01, 1.0, ['sigmoid', 'sigmoid'], \"logistic\", 'class', 80, early_stop=-1, \\\n",
    "             regularization='l2', reg_param = 0.0, batch_size = 10)\n",
    "nt.fit(x, y)\n",
    "\n",
    "ypred = nt.predict(xtest)\n",
    "print accuracy_score(ypred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = misc.imread('data/big_alphabet_29x29/mutant-0-0-0.bmp', flatten='grey')\n",
    "alphabet_size = 26\n",
    "im_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for letter in range(alphabet_size):\n",
    "    for i in range(9):\n",
    "        path = \"data/big_alphabet_29x29/mutant-\" + str(letter) + \"-\" + str(i) + \"-0.bmp\"\n",
    "        im = misc.imread(path, flatten='grey')\n",
    "        if (i == 0 or i == 1):\n",
    "            x_test.append(im.reshape(im_size * im_size))\n",
    "            x_test[-1] /= 255\n",
    "            y_test.append(letter)\n",
    "        else:\n",
    "            x.append(im.reshape(im_size * im_size))\n",
    "            x[-1] /= 255\n",
    "            y.append(letter)\n",
    "for letter in range(alphabet_size):\n",
    "    path = \"data/big_alphabet_29x29/class-\" + str(letter) + \".bmp\"\n",
    "    im = misc.imread(path, flatten='grey')\n",
    "    x_test.append(im.reshape(im_size * im_size))\n",
    "    x_test[-1] /= 255\n",
    "    y_test.append(letter)\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "#ytest = np.arange(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 841) (182,) (78, 841) (78,)\n"
     ]
    }
   ],
   "source": [
    "print x.shape, y.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.0\n",
      "2 0.0526315789474\n",
      "3 0.0526315789474\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n",
      "9 0.0\n",
      "10 0.0\n",
      "11 0.0\n",
      "12 0.0\n",
      "13 0.0\n",
      "14 0.0\n",
      "15 0.0\n",
      "16 0.0\n",
      "17 0.0\n",
      "18 0.0\n",
      "19 0.0\n",
      "20 0.0\n",
      "21 0.0\n",
      "22 0.0\n",
      "23 0.0\n",
      "24 0.0\n",
      "25 0.0\n",
      "26 0.0\n",
      "27 0.0\n",
      "28 0.0\n",
      "29 0.0\n",
      "30 0.0\n",
      "31 0.0\n",
      "32 0.0\n",
      "33 0.0\n",
      "34 0.0\n",
      "35 0.0\n",
      "36 0.0\n",
      "37 0.0\n",
      "38 0.0\n",
      "39 0.0\n",
      "40 0.0\n",
      "41 0.0\n",
      "42 0.0\n",
      "43 0.0\n",
      "44 0.0\n",
      "45 0.0\n",
      "46 0.0\n",
      "47 0.0\n",
      "48 0.0\n",
      "49 0.0\n",
      "50 0.0\n",
      "51 0.0\n",
      "52 0.0\n",
      "53 0.0\n",
      "54 0.0\n",
      "55 0.0\n",
      "56 0.0\n",
      "57 0.0\n",
      "58 0.0\n",
      "59 0.0\n",
      "60 0.0\n",
      "61 0.0\n",
      "62 0.0\n",
      "63 0.0\n",
      "64 0.0\n",
      "65 0.0\n",
      "66 0.0\n",
      "67 0.0\n",
      "68 0.0\n",
      "69 0.0\n",
      "70 0.0\n",
      "71 0.0\n",
      "72 0.0\n",
      "73 0.0\n",
      "74 0.0\n",
      "75 0.0\n",
      "76 0.0\n",
      "77 0.0\n",
      "78 0.0\n",
      "79 0.0\n",
      "80 0.0\n",
      "81 0.0\n",
      "82 0.0\n",
      "83 0.0\n",
      "84 0.0\n",
      "85 0.0\n",
      "86 0.0\n",
      "87 0.0\n",
      "88 0.0\n",
      "89 0.0\n",
      "90 0.0\n",
      "91 0.0\n",
      "92 0.0\n",
      "93 0.0\n",
      "94 0.0\n",
      "95 0.0\n",
      "96 0.0\n",
      "97 0.0\n",
      "98 0.0\n",
      "99 0.0\n",
      "100 0.0\n",
      "101 0.0\n",
      "102 0.0\n",
      "103 0.0\n",
      "104 0.0\n",
      "105 0.0\n",
      "106 0.0\n",
      "107 0.0526315789474\n",
      "108 0.0\n",
      "109 0.0\n",
      "110 0.0526315789474\n",
      "111 0.0526315789474\n",
      "112 0.0526315789474\n",
      "113 0.0526315789474\n",
      "114 0.0526315789474\n",
      "115 0.0526315789474\n",
      "116 0.0526315789474\n",
      "117 0.0526315789474\n",
      "118 0.0526315789474\n",
      "119 0.0526315789474\n",
      "120 0.0526315789474\n",
      "121 0.0526315789474\n",
      "122 0.0526315789474\n",
      "123 0.0526315789474\n",
      "124 0.0526315789474\n",
      "125 0.0526315789474\n",
      "126 0.0526315789474\n",
      "127 0.0526315789474\n",
      "128 0.0526315789474\n",
      "129 0.0526315789474\n",
      "130 0.0526315789474\n",
      "131 0.0526315789474\n",
      "132 0.0526315789474\n",
      "133 0.0526315789474\n",
      "134 0.0526315789474\n",
      "135 0.0526315789474\n",
      "136 0.0526315789474\n",
      "137 0.0526315789474\n",
      "138 0.0526315789474\n",
      "139 0.0526315789474\n",
      "140 0.0526315789474\n",
      "141 0.0526315789474\n",
      "142 0.0526315789474\n",
      "143 0.105263157895\n",
      "144 0.0526315789474\n",
      "145 0.0526315789474\n",
      "146 0.105263157895\n",
      "147 0.105263157895\n",
      "148 0.105263157895\n",
      "149 0.105263157895\n",
      "150 0.105263157895\n",
      "151 0.105263157895\n",
      "152 0.105263157895\n",
      "153 0.105263157895\n",
      "154 0.105263157895\n",
      "155 0.105263157895\n",
      "156 0.105263157895\n",
      "157 0.105263157895\n",
      "158 0.105263157895\n",
      "159 0.105263157895\n",
      "160 0.105263157895\n",
      "161 0.105263157895\n",
      "162 0.105263157895\n",
      "163 0.105263157895\n",
      "164 0.105263157895\n",
      "165 0.105263157895\n",
      "166 0.105263157895\n",
      "167 0.105263157895\n",
      "168 0.105263157895\n",
      "169 0.105263157895\n",
      "170 0.105263157895\n",
      "171 0.105263157895\n",
      "172 0.105263157895\n",
      "173 0.157894736842\n",
      "174 0.105263157895\n",
      "175 0.105263157895\n",
      "176 0.105263157895\n",
      "177 0.105263157895\n",
      "178 0.157894736842\n",
      "179 0.157894736842\n",
      "180 0.105263157895\n",
      "181 0.105263157895\n",
      "182 0.105263157895\n",
      "183 0.157894736842\n",
      "184 0.105263157895\n",
      "185 0.105263157895\n",
      "186 0.105263157895\n",
      "187 0.210526315789\n",
      "188 0.157894736842\n",
      "189 0.157894736842\n",
      "190 0.157894736842\n",
      "191 0.157894736842\n",
      "192 0.210526315789\n",
      "193 0.157894736842\n",
      "194 0.157894736842\n",
      "195 0.157894736842\n",
      "196 0.157894736842\n",
      "197 0.210526315789\n",
      "198 0.210526315789\n",
      "199 0.210526315789\n",
      "200 0.210526315789\n",
      "201 0.210526315789\n",
      "202 0.210526315789\n",
      "203 0.210526315789\n",
      "204 0.263157894737\n",
      "205 0.210526315789\n",
      "206 0.210526315789\n",
      "207 0.210526315789\n",
      "208 0.263157894737\n",
      "209 0.315789473684\n",
      "210 0.263157894737\n",
      "211 0.315789473684\n",
      "212 0.210526315789\n",
      "213 0.263157894737\n",
      "214 0.263157894737\n",
      "215 0.263157894737\n",
      "216 0.210526315789\n",
      "217 0.315789473684\n",
      "218 0.315789473684\n",
      "219 0.263157894737\n",
      "220 0.315789473684\n",
      "221 0.315789473684\n",
      "222 0.315789473684\n",
      "223 0.315789473684\n",
      "224 0.315789473684\n",
      "225 0.315789473684\n",
      "226 0.315789473684\n",
      "227 0.315789473684\n",
      "228 0.315789473684\n",
      "229 0.315789473684\n",
      "230 0.263157894737\n",
      "231 0.315789473684\n",
      "232 0.315789473684\n",
      "233 0.315789473684\n",
      "234 0.315789473684\n",
      "235 0.368421052632\n",
      "236 0.315789473684\n",
      "237 0.315789473684\n",
      "238 0.315789473684\n",
      "239 0.315789473684\n",
      "240 0.315789473684\n",
      "241 0.368421052632\n",
      "242 0.368421052632\n",
      "243 0.315789473684\n",
      "244 0.368421052632\n",
      "245 0.315789473684\n",
      "246 0.368421052632\n",
      "247 0.368421052632\n",
      "248 0.315789473684\n",
      "249 0.368421052632\n",
      "250 0.315789473684\n",
      "251 0.315789473684\n",
      "252 0.368421052632\n",
      "253 0.315789473684\n",
      "254 0.315789473684\n",
      "255 0.368421052632\n",
      "256 0.368421052632\n",
      "257 0.368421052632\n",
      "258 0.368421052632\n",
      "259 0.368421052632\n",
      "260 0.368421052632\n",
      "261 0.368421052632\n",
      "262 0.368421052632\n",
      "263 0.368421052632\n",
      "264 0.368421052632\n",
      "265 0.368421052632\n",
      "266 0.421052631579\n",
      "267 0.368421052632\n",
      "268 0.368421052632\n",
      "269 0.368421052632\n",
      "270 0.368421052632\n",
      "271 0.421052631579\n",
      "272 0.421052631579\n",
      "273 0.473684210526\n",
      "274 0.473684210526\n",
      "275 0.368421052632\n",
      "276 0.473684210526\n",
      "277 0.473684210526\n",
      "278 0.421052631579\n",
      "279 0.421052631579\n",
      "280 0.473684210526\n",
      "281 0.421052631579\n",
      "282 0.473684210526\n",
      "283 0.473684210526\n",
      "284 0.473684210526\n",
      "285 0.473684210526\n",
      "286 0.473684210526\n",
      "287 0.473684210526\n",
      "288 0.473684210526\n",
      "289 0.473684210526\n",
      "290 0.473684210526\n",
      "291 0.473684210526\n",
      "292 0.473684210526\n",
      "293 0.473684210526\n",
      "294 0.473684210526\n",
      "295 0.473684210526\n",
      "296 0.526315789474\n",
      "297 0.473684210526\n",
      "298 0.473684210526\n",
      "299 0.473684210526\n",
      "300 0.526315789474\n",
      "301 0.526315789474\n",
      "302 0.473684210526\n",
      "303 0.473684210526\n",
      "304 0.473684210526\n",
      "305 0.526315789474\n",
      "306 0.526315789474\n",
      "307 0.578947368421\n",
      "308 0.526315789474\n",
      "309 0.578947368421\n",
      "310 0.526315789474\n",
      "311 0.526315789474\n",
      "312 0.578947368421\n",
      "313 0.578947368421\n",
      "314 0.578947368421\n",
      "315 0.578947368421\n",
      "316 0.526315789474\n",
      "317 0.578947368421\n",
      "318 0.578947368421\n",
      "319 0.631578947368\n",
      "320 0.578947368421\n",
      "321 0.578947368421\n",
      "322 0.631578947368\n",
      "323 0.684210526316\n",
      "324 0.631578947368\n",
      "325 0.631578947368\n",
      "326 0.684210526316\n",
      "327 0.631578947368\n",
      "328 0.684210526316\n",
      "329 0.631578947368\n",
      "330 0.684210526316\n",
      "331 0.684210526316\n",
      "332 0.684210526316\n",
      "333 0.684210526316\n",
      "334 0.684210526316\n",
      "335 0.684210526316\n",
      "336 0.684210526316\n",
      "337 0.684210526316\n",
      "338 0.684210526316\n",
      "339 0.736842105263\n",
      "340 0.684210526316\n",
      "341 0.684210526316\n",
      "342 0.684210526316\n",
      "343 0.684210526316\n",
      "344 0.684210526316\n",
      "345 0.736842105263\n",
      "346 0.684210526316\n",
      "347 0.684210526316\n",
      "348 0.684210526316\n",
      "349 0.684210526316\n",
      "350 0.684210526316\n",
      "351 0.684210526316\n",
      "352 0.736842105263\n",
      "353 0.736842105263\n",
      "354 0.684210526316\n",
      "355 0.736842105263\n",
      "356 0.684210526316\n",
      "357 0.736842105263\n",
      "358 0.684210526316\n",
      "359 0.684210526316\n",
      "360 0.736842105263\n",
      "361 0.736842105263\n",
      "362 0.736842105263\n",
      "363 0.736842105263\n",
      "364 0.736842105263\n",
      "365 0.736842105263\n",
      "366 0.684210526316\n",
      "367 0.684210526316\n",
      "368 0.684210526316\n",
      "369 0.736842105263\n",
      "370 0.736842105263\n",
      "371 0.736842105263\n",
      "372 0.736842105263\n",
      "373 0.736842105263\n",
      "374 0.736842105263\n",
      "375 0.736842105263\n",
      "376 0.736842105263\n",
      "377 0.736842105263\n",
      "378 0.736842105263\n",
      "379 0.736842105263\n",
      "380 0.736842105263\n",
      "381 0.736842105263\n",
      "382 0.789473684211\n",
      "383 0.789473684211\n",
      "384 0.684210526316\n",
      "385 0.736842105263\n",
      "386 0.736842105263\n",
      "387 0.736842105263\n",
      "388 0.789473684211\n",
      "389 0.789473684211\n",
      "390 0.789473684211\n",
      "391 0.736842105263\n",
      "392 0.789473684211\n",
      "393 0.842105263158\n",
      "394 0.842105263158\n",
      "395 0.789473684211\n",
      "396 0.842105263158\n",
      "397 0.789473684211\n",
      "398 0.789473684211\n",
      "399 0.842105263158\n",
      "400 0.789473684211\n",
      "401 0.789473684211\n",
      "402 0.842105263158\n",
      "403 0.789473684211\n",
      "404 0.842105263158\n",
      "405 0.842105263158\n",
      "406 0.842105263158\n",
      "407 0.789473684211\n",
      "408 0.789473684211\n",
      "409 0.842105263158\n",
      "410 0.842105263158\n",
      "411 0.842105263158\n",
      "412 0.842105263158\n",
      "413 0.842105263158\n",
      "414 0.842105263158\n",
      "415 0.842105263158\n",
      "416 0.842105263158\n",
      "417 0.842105263158\n",
      "418 0.842105263158\n",
      "419 0.842105263158\n",
      "420 0.842105263158\n",
      "421 0.842105263158\n",
      "422 0.842105263158\n",
      "423 0.842105263158\n",
      "424 0.842105263158\n",
      "425 0.842105263158\n",
      "426 0.842105263158\n",
      "427 0.842105263158\n",
      "428 0.842105263158\n",
      "429 0.789473684211\n",
      "430 0.842105263158\n",
      "431 0.894736842105\n",
      "432 0.842105263158\n",
      "433 0.894736842105\n",
      "434 0.789473684211\n",
      "435 0.842105263158\n",
      "436 0.894736842105\n",
      "437 0.789473684211\n",
      "438 0.789473684211\n",
      "439 0.842105263158\n",
      "440 0.894736842105\n",
      "441 0.789473684211\n",
      "442 0.842105263158\n",
      "443 0.894736842105\n",
      "444 0.842105263158\n",
      "445 0.894736842105\n",
      "446 0.894736842105\n",
      "447 0.842105263158\n",
      "448 0.894736842105\n",
      "449 0.894736842105\n",
      "450 0.842105263158\n",
      "451 0.894736842105\n",
      "452 0.789473684211\n",
      "453 0.894736842105\n",
      "454 0.894736842105\n",
      "455 0.894736842105\n",
      "456 0.842105263158\n",
      "457 0.842105263158\n",
      "458 0.842105263158\n",
      "459 0.894736842105\n",
      "460 0.842105263158\n",
      "461 0.842105263158\n",
      "462 0.842105263158\n",
      "463 0.842105263158\n",
      "464 0.894736842105\n",
      "465 0.894736842105\n",
      "466 0.894736842105\n",
      "467 0.842105263158\n",
      "468 0.842105263158\n",
      "469 0.894736842105\n",
      "470 0.894736842105\n",
      "471 0.894736842105\n",
      "472 0.842105263158\n",
      "473 0.842105263158\n",
      "474 0.842105263158\n",
      "475 0.842105263158\n",
      "476 0.894736842105\n",
      "477 0.894736842105\n",
      "478 0.842105263158\n",
      "479 0.842105263158\n",
      "480 0.842105263158\n",
      "481 0.947368421053\n",
      "482 0.947368421053\n",
      "483 0.894736842105\n",
      "484 0.947368421053\n",
      "485 0.947368421053\n",
      "486 0.894736842105\n",
      "487 0.947368421053\n",
      "488 0.894736842105\n",
      "489 0.894736842105\n",
      "490 0.947368421053\n",
      "491 0.947368421053\n",
      "492 0.894736842105\n",
      "493 0.894736842105\n",
      "494 0.947368421053\n",
      "495 0.947368421053\n",
      "496 0.947368421053\n",
      "497 0.947368421053\n",
      "498 0.947368421053\n",
      "499 0.894736842105\n",
      "500 0.947368421053\n",
      "501 0.894736842105\n",
      "502 0.947368421053\n",
      "503 0.894736842105\n",
      "504 0.947368421053\n",
      "505 0.894736842105\n",
      "506 0.947368421053\n",
      "507 0.947368421053\n",
      "508 0.894736842105\n",
      "509 0.947368421053\n",
      "510 0.894736842105\n",
      "511 0.947368421053\n",
      "512 0.947368421053\n",
      "513 0.894736842105\n",
      "514 0.894736842105\n",
      "515 0.894736842105\n",
      "516 0.894736842105\n",
      "517 0.894736842105\n",
      "518 0.894736842105\n",
      "519 0.894736842105\n",
      "520 0.894736842105\n",
      "521 0.894736842105\n",
      "522 0.894736842105\n",
      "523 0.947368421053\n",
      "524 0.894736842105\n",
      "525 0.947368421053\n",
      "526 0.947368421053\n",
      "527 0.947368421053\n",
      "528 0.894736842105\n",
      "529 0.894736842105\n",
      "530 0.947368421053\n",
      "531 0.947368421053\n",
      "532 0.947368421053\n",
      "533 0.947368421053\n",
      "534 0.894736842105\n",
      "535 0.894736842105\n",
      "536 0.894736842105\n",
      "537 0.947368421053\n",
      "538 0.894736842105\n",
      "539 0.894736842105\n",
      "540 0.947368421053\n",
      "541 0.894736842105\n",
      "542 0.894736842105\n",
      "543 0.894736842105\n",
      "544 0.894736842105\n",
      "545 0.947368421053\n",
      "546 0.894736842105\n",
      "547 0.947368421053\n",
      "548 0.947368421053\n",
      "549 0.894736842105\n",
      "550 0.947368421053\n",
      "551 0.894736842105\n",
      "552 0.947368421053\n",
      "553 0.894736842105\n",
      "554 0.894736842105\n",
      "555 0.894736842105\n",
      "556 0.947368421053\n",
      "557 0.894736842105\n",
      "558 0.947368421053\n",
      "559 0.894736842105\n",
      "560 1.0\n",
      "561 0.894736842105\n",
      "562 1.0\n",
      "563 0.947368421053\n",
      "564 0.947368421053\n",
      "565 0.947368421053\n",
      "566 0.894736842105\n",
      "567 0.894736842105\n",
      "568 0.947368421053\n",
      "569 0.947368421053\n",
      "570 0.894736842105\n",
      "571 0.894736842105\n",
      "572 0.894736842105\n",
      "573 0.894736842105\n",
      "574 0.947368421053\n",
      "575 1.0\n",
      "576 0.894736842105\n",
      "577 1.0\n",
      "578 1.0\n",
      "579 1.0\n",
      "580 0.947368421053\n",
      "581 0.947368421053\n",
      "582 0.947368421053\n",
      "583 0.947368421053\n",
      "584 0.947368421053\n",
      "585 1.0\n",
      "586 0.947368421053\n",
      "587 0.947368421053\n",
      "588 1.0\n",
      "589 0.947368421053\n",
      "590 0.947368421053\n",
      "591 0.947368421053\n",
      "592 1.0\n",
      "593 1.0\n",
      "594 0.947368421053\n",
      "595 0.947368421053\n",
      "596 1.0\n",
      "597 1.0\n",
      "598 0.947368421053\n",
      "599 1.0\n",
      "600 0.947368421053\n",
      "601 0.947368421053\n",
      "602 0.947368421053\n",
      "603 0.947368421053\n",
      "604 1.0\n",
      "605 0.947368421053\n",
      "606 0.947368421053\n",
      "607 0.947368421053\n",
      "608 0.947368421053\n",
      "609 0.947368421053\n",
      "610 1.0\n",
      "611 0.947368421053\n",
      "612 0.947368421053\n",
      "613 0.947368421053\n",
      "614 0.947368421053\n",
      "615 0.947368421053\n",
      "616 0.947368421053\n",
      "617 1.0\n",
      "618 1.0\n",
      "619 1.0\n",
      "620 1.0\n",
      "621 1.0\n",
      "622 0.947368421053\n",
      "623 1.0\n",
      "624 0.947368421053\n",
      "625 0.947368421053\n",
      "626 0.947368421053\n",
      "627 1.0\n",
      "628 0.947368421053\n",
      "629 0.947368421053\n",
      "630 1.0\n",
      "631 1.0\n",
      "632 0.947368421053\n",
      "633 1.0\n",
      "634 0.947368421053\n",
      "635 1.0\n",
      "636 1.0\n",
      "637 0.947368421053\n",
      "638 1.0\n",
      "639 0.947368421053\n",
      "640 1.0\n",
      "641 0.947368421053\n",
      "642 0.947368421053\n",
      "643 1.0\n",
      "644 0.947368421053\n",
      "645 1.0\n",
      "646 1.0\n",
      "647 1.0\n",
      "648 0.947368421053\n",
      "649 1.0\n",
      "650 1.0\n",
      "651 1.0\n",
      "652 1.0\n",
      "653 1.0\n",
      "654 1.0\n",
      "655 1.0\n",
      "656 1.0\n",
      "657 1.0\n",
      "658 0.947368421053\n",
      "659 1.0\n",
      "660 1.0\n",
      "661 0.947368421053\n",
      "662 0.947368421053\n",
      "663 0.947368421053\n",
      "664 1.0\n",
      "665 1.0\n",
      "666 1.0\n",
      "667 1.0\n",
      "668 1.0\n",
      "669 1.0\n",
      "670 1.0\n",
      "671 1.0\n",
      "672 0.947368421053\n",
      "673 1.0\n",
      "674 0.947368421053\n",
      "675 1.0\n",
      "676 1.0\n",
      "677 1.0\n",
      "678 1.0\n",
      "679 1.0\n",
      "680 1.0\n",
      "681 1.0\n",
      "682 1.0\n",
      "683 1.0\n",
      "684 1.0\n",
      "685 1.0\n",
      "686 1.0\n",
      "687 1.0\n",
      "688 1.0\n",
      "689 1.0\n",
      "690 1.0\n",
      "691 1.0\n",
      "692 1.0\n",
      "693 1.0\n",
      "694 1.0\n",
      "695 1.0\n",
      "696 1.0\n",
      "697 1.0\n",
      "698 1.0\n",
      "699 1.0\n",
      "700 1.0\n",
      "701 1.0\n",
      "702 1.0\n",
      "703 1.0\n",
      "704 1.0\n",
      "705 1.0\n",
      "706 1.0\n",
      "707 1.0\n",
      "708 1.0\n",
      "709 1.0\n",
      "710 1.0\n",
      "711 1.0\n",
      "712 1.0\n",
      "713 1.0\n",
      "714 1.0\n",
      "715 1.0\n",
      "716 1.0\n",
      "717 1.0\n",
      "718 1.0\n",
      "719 1.0\n",
      "720 1.0\n",
      "721 1.0\n",
      "722 1.0\n",
      "723 1.0\n",
      "724 1.0\n",
      "725 1.0\n",
      "726 1.0\n",
      "727 1.0\n",
      "728 1.0\n",
      "729 1.0\n",
      "730 1.0\n",
      "731 1.0\n",
      "732 1.0\n",
      "733 1.0\n",
      "734 1.0\n",
      "735 1.0\n",
      "736 1.0\n",
      "737 1.0\n",
      "738 1.0\n",
      "739 1.0\n",
      "740 1.0\n",
      "741 1.0\n",
      "742 1.0\n",
      "743 1.0\n",
      "744 1.0\n",
      "745 1.0\n",
      "746 1.0\n",
      "747 1.0\n",
      "748 1.0\n",
      "749 1.0\n",
      "750 1.0\n",
      "751 1.0\n",
      "752 1.0\n",
      "753 1.0\n",
      "754 1.0\n",
      "755 1.0\n",
      "756 1.0\n",
      "757 1.0\n",
      "758 1.0\n",
      "759 1.0\n",
      "760 1.0\n",
      "761 1.0\n",
      "762 1.0\n",
      "763 1.0\n",
      "764 1.0\n",
      "765 1.0\n",
      "766 1.0\n",
      "767 1.0\n",
      "768 1.0\n",
      "769 1.0\n",
      "770 1.0\n",
      "771 1.0\n",
      "772 1.0\n",
      "773 1.0\n",
      "774 1.0\n",
      "775 1.0\n",
      "776 1.0\n",
      "777 1.0\n",
      "778 1.0\n",
      "779 1.0\n",
      "780 1.0\n",
      "781 1.0\n",
      "782 1.0\n",
      "783 1.0\n",
      "784 1.0\n",
      "785 1.0\n",
      "786 1.0\n",
      "787 1.0\n",
      "788 1.0\n",
      "789 1.0\n",
      "790 1.0\n",
      "791 1.0\n",
      "792 1.0\n",
      "793 1.0\n",
      "794 1.0\n",
      "795 1.0\n",
      "796 1.0\n",
      "797 1.0\n",
      "798 1.0\n",
      "799 1.0\n",
      "[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nt = network(layers_list=[25, alphabet_size, x.shape[1]], learning_rate=0.01, alpha=1.0, \\\n",
    "             activation_functions=['sigmoid', 'sigmoid','x'], cost_func=\"logistic\", \\\n",
    "             mode='class', cou_iter=800, early_stop=-1, regularization = 'l2', reg_param = 0.05, batch_size = 10)\n",
    "nt.fit(x, y)\n",
    "\n",
    "ypred = nt.predict(x_test)\n",
    "print ypred\n",
    "print accuracy_score(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11 12\n",
      " 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24\n",
      " 25 25  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      " 23 24 25]\n"
     ]
    }
   ],
   "source": [
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
