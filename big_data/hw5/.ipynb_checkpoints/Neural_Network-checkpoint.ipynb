{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "import theano.tensor as th\n",
    "from scipy import misc\n",
    "import copy\n",
    "from numpy.random import uniform\n",
    "from numpy.random import normal\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self, layers_list, learning_rate, alpha, activation_functions, cost_func, mode):\n",
    "        self.layers_count = len(layers_list)\n",
    "        self.weight_list = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers_list = layers_list\n",
    "        self.alpha = alpha\n",
    "        self.cost_func = cost_func\n",
    "        self.activation_functions = activation_functions\n",
    "        self.mode = mode\n",
    "        for i in range(1, self.layers_count):\n",
    "            m = np.asarray(normal(0, 0.15, (self.layers_list[i - 1] + 1, self.layers_list[i])))\n",
    "            self.weight_list.append(m)\n",
    "            \n",
    "    def w_out(self):\n",
    "        for i in range(len(self.weight_list)):\n",
    "            print self.weight_list[i]\n",
    "            print '\\n'\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        val = one / (one + np.exp(-x * self.alpha))\n",
    "        #for i in range(len(val)):\n",
    "        #    if (val[i] == 0):\n",
    "        #        val[i] = 0.00001\n",
    "        #    elif (val[i] == 1):\n",
    "        #        val[i] = 0.99999\n",
    "        return one / (one + np.exp(-x * self.alpha))\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        return (one - self.sigmoid(x)) * self.sigmoid(x) * self.alpha\n",
    "    \n",
    "    def x(self, x):\n",
    "        return x\n",
    "    \n",
    "    def der_x(self, x):\n",
    "        return np.ones(len(x))\n",
    "    \n",
    "    def hyp_tg(self, x):\n",
    "        return (np.exp(self.alpha * x) - np.exp(-self.alpha * x)) / (np.exp(self.alpha * x) + np.exp(-self.alpha * x))\n",
    "    \n",
    "    def der_hyp_tg(self, x):\n",
    "        return self.alpha * (1 - (self.hyp_tg(x)) ** 2)\n",
    "    \n",
    "    def logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z\n",
    "            val = -np.sum(y_true * np.log(y_pred) + (one - y_true) * np.log(one - y_pred))\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return val\n",
    "    \n",
    "    def der_logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z\n",
    "            val = ((one - y_true) / (one - y_pred) - y_true / y_pred)\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = ((1.0 - y_true) / (1.0 - y_pred) - y_true / y_pred)\n",
    "        return val\n",
    "    \n",
    "    def square_cost(self, y_true, y_pred):\n",
    "        return  0.5*((y_true - y_pred) ** 2.0)\n",
    "    \n",
    "    def der_square_cost(self, y_true, y_pred):\n",
    "        return (y_pred - y_true)\n",
    "    \n",
    "    def add_first_layer(self, x_shape):\n",
    "        self.weight_list.insert(0, np.asarray(uniform(-1, 1, (x_shape, self.layers_list[0]))))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x)\n",
    "        x = np.insert(x, 0, [-1], axis = 1)\n",
    "        ans = []\n",
    "        for s_num, sample in enumerate(x):\n",
    "            v = sample\n",
    "            for num, layer in enumerate(self.weight_list):\n",
    "                v = np.dot(layer.T, v)\n",
    "                if (self.activation_functions[num] == 'sigmoid'):\n",
    "                    v = self.sigmoid(v)\n",
    "                elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                    v = self.hyp_tg(v)\n",
    "                elif (self.activation_functions[num] == 'x'):\n",
    "                    v = self.x(v)\n",
    "                    \n",
    "                if (num != self.layers_count - 1):\n",
    "                    v = np.insert(v, 0, [-1])\n",
    "                    \n",
    "            if (self.mode == 'class'):\n",
    "                ans.append(np.argmax(v))\n",
    "            elif (self.mode == 'reg'):\n",
    "                ans.append(v)\n",
    "        return ans\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        y = np.asarray(y)\n",
    "        x = np.insert(np.asarray(x), 0, -1, axis = 1)\n",
    "        self.add_first_layer(x.shape[1])\n",
    "        cou_iter = 100\n",
    "        k = 0\n",
    "        total_err = 0.0\n",
    "        prev_err = 0.0\n",
    "        while (k < cou_iter):\n",
    "            print total_err\n",
    "            total_err = 0.0\n",
    "            for s_num, sample in enumerate(x):\n",
    "                neuron_sum = []\n",
    "                neuron_out = []\n",
    "                neuron_out.append(np.asarray(sample, dtype = np.float))\n",
    "                v = sample\n",
    "                for num, layer in enumerate(self.weight_list):\n",
    "                    v = np.dot(layer.T, v)\n",
    "                    neuron_sum.append(v)\n",
    "                    if (self.activation_functions[num] == 'sigmoid'):\n",
    "                        v = self.sigmoid(v)\n",
    "                    elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                        v = self.hyp_tg(v)\n",
    "                    elif(self.activation_functions[num] == 'x'):\n",
    "                        v = self.x(v)\n",
    "                    if (num != self.layers_count - 1):\n",
    "                        v = np.insert(v, 0, [-1])\n",
    "                    neuron_out.append(v)\n",
    "                    \n",
    "            \n",
    "            der = np.zeros(1)\n",
    "            prev_layer = np.zeros(1)\n",
    "            \n",
    "            if (self.cost_func == \"square\"):\n",
    "                total_err += self.square_cost(y[s_num], v)\n",
    "\n",
    "            \n",
    "            for layer_num in range(self.layers_count - 1, -1, -1):\n",
    "                if (layer_num == self.layers_count -1):\n",
    "\n",
    "                    if (self.cost_func == 'square' and self.activation_functions[-1] == 'x'):\n",
    "                        der = self.der_square_cost(y[s_num], neuron_out[-1]) * self.der_x(neuron_sum[-1])\n",
    "                    elif (self.cost_func == 'logistic' and self.activation_functions[-1] == 'sigmoid'):\n",
    "                        der = self.der_logistic_cost(y[s_num], neuron_out[-1]) * self.der_sigmoid(neuron_sum[-1])\n",
    "                        \n",
    "                    prev_layer = self.weight_list[layer_num].copy()\n",
    "                    #print np.dot(neuron_out[layer_num].T.reshape(-1, 1), der.reshape(1, -1))\n",
    "                    #return 0\n",
    "                    self.weight_list[layer_num] -= self.learning_rate * np.dot(neuron_out[layer_num].T.reshape(-1, 1), der.reshape(1, -1))\n",
    "                else:\n",
    "                    if (self.activation_functions[layer_num] == 'sigmoid'):\n",
    "                        der = np.dot(prev_layer[1:], der) * self.der_sigmoid(neuron_sum[layer_num])\n",
    "                    elif (self.activation_functions[layer_num] == 'hyp_tg'):\n",
    "                        der = np.dot(prev_layer[1:], der) * self.der_hyp_tg(neuron_sum[layer_num])\n",
    "\n",
    "                    prev_layer = self.weight_list[layer_num].copy()\n",
    "                    self.weight_list[layer_num] -= self.learning_rate * (np.tile(neuron_out[layer_num], (len(der), 1))).T * np.tile(der, (len(neuron_out[layer_num]), 1))\n",
    "            #self.w_out()\n",
    "            #return 0\n",
    "            \n",
    "                \n",
    "                \n",
    "            k += 1\n",
    "\n",
    "            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[ 0.06333896]\n",
      "[ 0.04288096]\n",
      "[ 0.0291584]\n",
      "[ 0.01989455]\n",
      "[ 0.01360976]\n",
      "[ 0.00932967]\n",
      "[ 0.00640611]\n",
      "[ 0.00440442]\n",
      "[ 0.00303136]\n",
      "[ 0.0020881]\n",
      "[ 0.00143933]\n",
      "[ 0.00099268]\n",
      "[ 0.00068494]\n",
      "[ 0.00047278]\n",
      "[ 0.00032643]\n",
      "[ 0.00022544]\n",
      "[ 0.00015573]\n",
      "[ 0.00010759]\n",
      "[  7.43429929e-05]\n",
      "[  5.13754325e-05]\n",
      "[  3.55068556e-05]\n",
      "[  2.45416136e-05]\n",
      "[  1.69637685e-05]\n",
      "[  1.17264090e-05]\n",
      "[  8.10638487e-06]\n",
      "[  5.60409597e-06]\n",
      "[  3.87433640e-06]\n",
      "[  2.67855292e-06]\n",
      "[  1.85187808e-06]\n",
      "[  1.28036041e-06]\n",
      "[  8.85234788e-07]\n",
      "[  6.12054387e-07]\n",
      "[  4.23180813e-07]\n",
      "[  2.92594116e-07]\n",
      "[  2.02305763e-07]\n",
      "[  1.39879302e-07]\n",
      "[  9.67165423e-08]\n",
      "[  6.68728479e-08]\n",
      "[  4.62381374e-08]\n",
      "[  3.19706933e-08]\n",
      "[  2.21057260e-08]\n",
      "[  1.52847503e-08]\n",
      "[  1.05684819e-08]\n",
      "[  7.30747685e-09]\n",
      "[  5.05269134e-09]\n",
      "[  3.49364271e-09]\n",
      "[  2.41565295e-09]\n",
      "[  1.67028611e-09]\n",
      "[  1.15490810e-09]\n",
      "[  7.98553795e-10]\n",
      "[  5.52155098e-10]\n",
      "[  3.81784355e-10]\n",
      "[  2.63982586e-10]\n",
      "[  1.82529271e-10]\n",
      "[  1.26208858e-10]\n",
      "[  8.72664321e-11]\n",
      "[  6.03399094e-11]\n",
      "[  4.17217130e-11]\n",
      "[  2.88482614e-11]\n",
      "[  1.99469817e-11]\n",
      "[  1.37922385e-11]\n",
      "[  9.53657324e-12]\n",
      "[  6.59401538e-12]\n",
      "[  4.55939877e-12]\n",
      "[  3.15257343e-12]\n",
      "[  2.17983116e-12]\n",
      "[  1.50723339e-12]\n",
      "[  1.04216904e-12]\n",
      "[  7.20602611e-13]\n",
      "[  4.98257106e-13]\n",
      "[  3.44517411e-13]\n",
      "[  2.38214861e-13]\n",
      "[  1.64712489e-13]\n",
      "[  1.13889638e-13]\n",
      "[  7.87484287e-14]\n",
      "[  5.44502129e-14]\n",
      "[  3.76493315e-14]\n",
      "[  2.60324448e-14]\n",
      "[  1.80000058e-14]\n",
      "[  1.24460154e-14]\n",
      "[  8.60573611e-15]\n",
      "[  5.95039390e-15]\n",
      "[  4.11437059e-15]\n",
      "[  2.84486131e-15]\n",
      "[  1.96706536e-15]\n",
      "[  1.36011767e-15]\n",
      "[  9.40446674e-16]\n",
      "[  6.50267228e-16]\n",
      "[  4.49624079e-16]\n",
      "[  3.10890356e-16]\n",
      "[  2.14963609e-16]\n",
      "[  1.48635531e-16]\n",
      "[  1.02773306e-16]\n",
      "[  7.10620972e-17]\n",
      "[  4.91355372e-17]\n",
      "[  3.39745257e-17]\n",
      "[  2.34915184e-17]\n",
      "[  1.62430950e-17]\n",
      "[  1.12312079e-17]\n",
      "[array([-0.01568093]), array([-0.02124129]), array([-0.13192935]), array([-0.12062148]), array([-0.1280335]), array([-0.1161559]), array([-0.1432453]), array([-0.10379583]), array([ 0.01300117]), array([-0.12934093]), array([-0.0121517]), array([ 0.02038972]), array([-0.0074053]), array([-0.14747791]), array([-0.11378387]), array([-0.14245162]), array([-0.12645792]), array([-0.11940093]), array([ 0.01995295]), array([-0.13492264])] [1 1 0 2 2 1 0 2 2 2 1 0 2 0 2 1 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "shuffle(x, y)\n",
    "\n",
    "x = x[:100]\n",
    "y = y[:100]\n",
    "\n",
    "\n",
    "\n",
    "xtest = x[80:]\n",
    "ytest = y[80:]\n",
    "x = x[:80]\n",
    "y = y[:80]\n",
    "\n",
    "nt = network([9, 1], 0.05, 1.0, ['sigmoid', 'x'], \"square\", 'reg')\n",
    "nt.fit(x, y)\n",
    "ypred = nt.predict(xtest)\n",
    "print ypred, ytest\n",
    "#print np.sum((ytest - ypred) ** 2.0) / len(ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = misc.imread('data/big_alphabet_29x29/mutant-0-0-0.bmp', flatten='grey')\n",
    "alphabet_size = 25\n",
    "im_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "x_test = []\n",
    "for letter in range(alphabet_size):\n",
    "    for i in range(8):\n",
    "        path = \"data/big_alphabet_29x29/mutant-\" + str(letter) + \"-\" + str(i) + \"-0.bmp\"\n",
    "        im = misc.imread(path, flatten='grey')\n",
    "        x.append(im.reshape(im_size * im_size))\n",
    "        y.append(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for letter in range(alphabet_size):\n",
    "    path = \"data/big_alphabet_29x29/class-\" + str(letter) + \".bmp\"\n",
    "    im = misc.imread(path, flatten='grey')\n",
    "    x_test.append(im.reshape(im_size * im_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 15,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 15,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 22,\n",
       " 24,\n",
       " 24]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nt = network([1000, 26], 0.1, 1.0, ['sigmoid', 'sigmoid'], \"logistic\", 'class')\n",
    "nt.fit(x, y)\n",
    "nt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
