{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "import theano.tensor as th\n",
    "from scipy import misc\n",
    "import copy\n",
    "from numpy.random import uniform\n",
    "from numpy.random import normal\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_classification\n",
    "from math import copysign\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self, layers_list, learning_rate, alpha, activation_functions, cost_func, mode, \\\n",
    "                 cou_iter, early_stop, regularization, reg_param, batch_size):\n",
    "        self.layers_count = len(layers_list)\n",
    "        self.weight_list = []\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers_list = layers_list\n",
    "        self.alpha = alpha\n",
    "        self.cost_func = cost_func\n",
    "        self.activation_functions = activation_functions\n",
    "        self.mode = mode\n",
    "        self.batch_size = batch_size\n",
    "        self.cou_iter = cou_iter\n",
    "        self.early_stop = early_stop\n",
    "        self.regularization = regularization\n",
    "        self.reg_param = reg_param\n",
    "        for i in range(1, self.layers_count):\n",
    "            m = np.asarray(normal(0, 0.15, (self.layers_list[i - 1] + 1, self.layers_list[i])))\n",
    "            self.weight_list.append(m)\n",
    "            \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        val = one / (one + np.exp(-x * self.alpha))\n",
    "        for i in range(len(val)):\n",
    "            if (val[i] == 0):\n",
    "                val[i] = 0.00001\n",
    "            elif (val[i] == 1):\n",
    "                val[i] = 0.99999\n",
    "        return one / (one + np.exp(-x * self.alpha))\n",
    "    \n",
    "    def der_sigmoid(self, x):\n",
    "        one = np.ones(len(x))\n",
    "        return (one - self.sigmoid(x)) * self.sigmoid(x) * self.alpha\n",
    "    \n",
    "    def x(self, x):\n",
    "        return x\n",
    "    \n",
    "    def der_x(self, x):\n",
    "        return np.ones(len(x))\n",
    "    \n",
    "    def hyp_tg(self, x):\n",
    "        return (np.exp(self.alpha * x) - np.exp(-self.alpha * x)) / (np.exp(self.alpha * x) + np.exp(-self.alpha * x))\n",
    "    \n",
    "    def der_hyp_tg(self, x):\n",
    "        return self.alpha * (1 - (self.hyp_tg(x)) ** 2)\n",
    "    \n",
    "    def logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z.copy()\n",
    "            val = -np.sum(y_true * np.log(y_pred) + (one - y_true) * np.log(one - y_pred))\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return val\n",
    "    \n",
    "    def der_logistic_cost(self, y_true, y_pred):\n",
    "        val = 0\n",
    "        if (self.mode == 'class'):\n",
    "            z = np.zeros(len(y_pred))\n",
    "            z[y_true] = 1\n",
    "            one = np.ones(len(y_pred))\n",
    "            y_true = z\n",
    "            val = ((one*1.0 - y_true*1.0) / (one - y_pred*1.0) - (y_true*1.0 / y_pred))\n",
    "        elif (self.mode == 'reg'):\n",
    "            val = ((1.0 - y_true) / (1.0 - y_pred) - y_true / y_pred)\n",
    "        return val\n",
    "    \n",
    "    def add_two_lists(self, l1, l2):\n",
    "        for pos in range(len(l1)):\n",
    "            l1[pos] += l2[pos]\n",
    "        return l1\n",
    "    \n",
    "    def div_list(self, l1, m):\n",
    "        for pos in range(len(l1)):\n",
    "            l1[pos] /= m\n",
    "        return l1\n",
    "    \n",
    "    def square_cost(self, y_true, y_pred):\n",
    "        return  0.5*((y_true - y_pred) ** 2.0)\n",
    "    \n",
    "    def der_square_cost(self, y_true, y_pred):\n",
    "        return (y_pred - y_true)\n",
    "    \n",
    "    def add_first_layer(self, x_shape):\n",
    "        self.weight_list.insert(0, np.asarray(normal(0, 0.15, (x_shape, self.layers_list[0]))))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        x = np.asarray(x)\n",
    "        x = np.insert(x, 0, [-1], axis = 1)\n",
    "        ans = []\n",
    "        for s_num, sample in enumerate(x):\n",
    "            v = sample\n",
    "            for num, layer in enumerate(self.weight_list):\n",
    "                v = np.dot(layer.T, v)\n",
    "                if (self.activation_functions[num] == 'sigmoid'):\n",
    "                    v = self.sigmoid(v)\n",
    "                elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                    v = self.hyp_tg(v)\n",
    "                elif (self.activation_functions[num] == 'x'):\n",
    "                    v = self.x(v)\n",
    "                if (num != self.layers_count - 1):\n",
    "                    v = np.insert(v, 0, [-1])\n",
    "            if (self.mode == 'class'):\n",
    "                ans.append(np.argmax(v))\n",
    "            elif (self.mode == 'reg'):\n",
    "                ans.append(v)\n",
    "        return ans\n",
    "    \n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        y = np.asarray(y)\n",
    "        x = np.insert(np.asarray(x), 0, -1, axis = 1)\n",
    "        self.add_first_layer(x.shape[1])\n",
    "        k = 0\n",
    "        total_err = np.inf\n",
    "        x_size = len(x)\n",
    "        ind_frag = x_size * 0.9\n",
    "        x, y = shuffle(x, y)\n",
    "        test_x = x[ind_frag:]\n",
    "        test_y = y[ind_frag:]\n",
    "        x_new = x[:ind_frag]\n",
    "        y_new = y[:ind_frag]\n",
    "        err_arr = []\n",
    "        result_matrix = []\n",
    "        while (k < self.cou_iter and (len(err_arr) < 10 or err_arr[-1] - err_arr[len(err_arr) - 9] >= self.early_stop)):\n",
    "            print k, total_err\n",
    "            total_err = 0.0\n",
    "            x_new, y_new = shuffle(x_new, y_new)\n",
    "            balance = self.batch_size\n",
    "            for s_num, sample in enumerate(x_new):\n",
    "                now_matrix = []\n",
    "                if (balance == 0):\n",
    "                    #print 'I here!!!'\n",
    "                    #return 0\n",
    "                    balance = self.batch_size\n",
    "                    self.weight_list = self.add_two_lists(self.weight_list, result_matrix)\n",
    "                    result_matrix = []\n",
    "                neuron_sum = []\n",
    "                neuron_out = []\n",
    "                neuron_out.append(np.asarray(sample, dtype = np.float))\n",
    "                v = sample\n",
    "                for num, layer in enumerate(self.weight_list):\n",
    "                    v = np.dot(layer.T, v)\n",
    "                    neuron_sum.append(v)\n",
    "                    if (self.activation_functions[num] == 'sigmoid'):\n",
    "                        v = self.sigmoid(v)\n",
    "                    elif (self.activation_functions[num] == 'hyp_tg'):\n",
    "                        v = self.hyp_tg(v)\n",
    "                    elif(self.activation_functions[num] == 'x'):\n",
    "                        v = self.x(v) \n",
    "                    if (num != self.layers_count - 1):\n",
    "                        v = np.insert(v, 0, [-1])\n",
    "                    neuron_out.append(v)\n",
    "                \n",
    "                if (neuron_out[-1].shape[0] == 1):\n",
    "                    neuron_out[-1] = neuron_out[-1][0]\n",
    "\n",
    "                if (self.cost_func == \"logistic\" and self.activation_functions[-1] == \"sigmoid\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_sigmoid(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == \"logistic\" and self.activation_functions[-1] == \"hyp_tg\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_hyp_tg(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == 'logistic' and self.activation_functions[-1] == \"x\"):\n",
    "                    der_neuron = [(self.der_logistic_cost(y_new[s_num], neuron_out[-1]) * self.der_x(neuron_sum[-1]))]\n",
    "                elif (self.cost_func == 'square' and self.activation_functions[-1] == \"x\"):\n",
    "                    der_neuron = [(self.der_square_cost(y_new[s_num], neuron_out[-1]) * self.der_x(neuron_sum[-1]))]\n",
    "                \n",
    "                num = 1\n",
    "                for layer in reversed(self.weight_list[1:]):\n",
    "                    if (self.activation_functions[self.layers_count - num - 1] == 'sigmoid'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_sigmoid(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    elif (self.activation_functions[self.layers_count - num - 1] == 'hyp_tg'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_hyp_tg(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    elif (self.activation_functions[self.layers_count - num - 1] == 'x'):\n",
    "                        der_neuron.insert(0, np.dot(layer[1:], der_neuron[0]) * self.der_x(neuron_sum[len(neuron_sum) - num - 1]))\n",
    "                    num += 1\n",
    "\n",
    "                for layer_num in range(self.layers_count):\n",
    "                    #if (self.regularization == 'l1'):\n",
    "                    #    self.weight_list[self.layers_count - layer_num - 1] -= self.learning_rate *( \\\n",
    "                    #    np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)) +\\\n",
    "                    #    self.reg_param * np.sign(self.weight_list[self.layers_count - layer_num - 1]))\n",
    "                        \n",
    "                    #elif (self.regularization == 'l2'):\n",
    "                    #    self.weight_list[self.layers_count - layer_num - 1] -= self.learning_rate *(\\\n",
    "                    #    np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)) +\\\n",
    "                    #    self.reg_param * self.weight_list[self.layers_count - layer_num - 1])\n",
    "                    #if (balance == self.batch_size):\n",
    "                    now_matrix.insert(0, -1.0*self.learning_rate* np.dot(neuron_out[len(neuron_out) - layer_num - 2].T.reshape(-1, 1), der_neuron[len(der_neuron) - layer_num - 1].reshape(1, -1)))\n",
    "                if (len(result_matrix) == 0):\n",
    "                    result_matrix = list(now_matrix)\n",
    "                else:\n",
    "                    result_matrix = self.add_two_lists(result_matrix, now_matrix)\n",
    "                balance -= 1\n",
    "                #print len(result_matrix)\n",
    "                \n",
    "            k += 1\n",
    "            y_pred = self.predict(test_x[:, 1:])\n",
    "            if (self.mode == 'class'):\n",
    "                total_err = accuracy_score(test_y, y_pred)\n",
    "            elif (self.mode == 'reg'):\n",
    "                total_err = mean_squared_error(test_y, y_pred)\n",
    "            err_arr.append(total_err)\n",
    "            #print result_matrix\n",
    "            #return 0\n",
    "            \n",
    "            \n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.0\n",
      "2 0.272727272727\n",
      "3 0.272727272727\n",
      "4 0.272727272727\n",
      "5 1.0\n",
      "6 1.0\n",
      "7 1.0\n",
      "8 1.0\n",
      "9 1.0\n",
      "10 0.272727272727\n",
      "11 0.909090909091\n",
      "12 0.909090909091\n",
      "13 1.0\n",
      "14 1.0\n",
      "15 1.0\n",
      "16 0.909090909091\n",
      "17 1.0\n",
      "18 0.909090909091\n",
      "19 1.0\n",
      "20 0.363636363636\n",
      "21 1.0\n",
      "22 1.0\n",
      "23 1.0\n",
      "24 1.0\n",
      "25 1.0\n",
      "26 0.727272727273\n",
      "27 0.909090909091\n",
      "28 1.0\n",
      "29 1.0\n",
      "30 0.818181818182\n",
      "31 0.909090909091\n",
      "32 0.909090909091\n",
      "33 1.0\n",
      "34 1.0\n",
      "35 0.909090909091\n",
      "36 1.0\n",
      "37 1.0\n",
      "38 0.909090909091\n",
      "39 0.909090909091\n",
      "40 0.909090909091\n",
      "41 1.0\n",
      "42 1.0\n",
      "43 1.0\n",
      "44 1.0\n",
      "45 0.636363636364\n",
      "46 1.0\n",
      "47 1.0\n",
      "48 0.636363636364\n",
      "49 1.0\n",
      "[1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1] [2 1 2 1 2 2 1 1 2 2 2 2 1 0 2 1 1 2 1 1 1 1 1 2 1 2 1 1 0 0 2 0 0 1 2 2 1\n",
      " 1 0 1]\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "x = iris.data\n",
    "y = iris.target\n",
    "\n",
    "x, y = shuffle(x, y)\n",
    "\n",
    "xtest = x[110:]\n",
    "ytest = y[110:]\n",
    "x = x[:110]\n",
    "y = y[:110]\n",
    "\n",
    "nt = network([10, 3], 0.05, 1.0, ['sigmoid', 'sigmoid'], \"logistic\", 'class', 50, early_stop=-1.0, \\\n",
    "             regularization='l2', reg_param = 0.0, batch_size = 5)\n",
    "nt.fit(x, y)\n",
    "ypred = nt.predict(xtest)\n",
    "print ypred, ytest\n",
    "print accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.466666666667\n",
      "3 0.422222222222\n",
      "4 0.422222222222\n",
      "5 0.488888888889\n",
      "6 0.488888888889\n",
      "7 0.488888888889\n",
      "8 0.488888888889\n",
      "9 0.511111111111\n",
      "10 0.511111111111\n",
      "11 0.488888888889\n",
      "12 0.511111111111\n",
      "13 0.488888888889\n",
      "14 0.488888888889\n",
      "15 0.533333333333\n",
      "16 0.488888888889\n",
      "17 0.466666666667\n",
      "18 0.466666666667\n",
      "19 0.511111111111\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "x, y = make_classification(n_samples=500, n_features=20, n_informative=10, n_classes=4)\n",
    "xtest = x[450:]\n",
    "ytest = y[450:]\n",
    "x = x[:450]\n",
    "y = y[:450]\n",
    "nt = network([20, 4], 0.01, 1.0, ['sigmoid', 'sigmoid'], \"logistic\", 'class', 20, early_stop=-1, \\\n",
    "             regularization='l2', reg_param = 0.0, batch_size = 10)\n",
    "nt.fit(x, y)\n",
    "\n",
    "ypred = nt.predict(xtest)\n",
    "print accuracy_score(ypred, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = misc.imread('data/big_alphabet_29x29/mutant-0-0-0.bmp', flatten='grey')\n",
    "alphabet_size = 26\n",
    "im_size = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "for letter in range(alphabet_size):\n",
    "    for i in range(9):\n",
    "        path = \"data/big_alphabet_29x29/mutant-\" + str(letter) + \"-\" + str(i) + \"-0.bmp\"\n",
    "        im = misc.imread(path, flatten='grey')\n",
    "        if (i == 0 or i == 1):\n",
    "            x_test.append(im.reshape(im_size * im_size))\n",
    "            x_test[-1] /= 255\n",
    "            y_test.append(letter)\n",
    "        else:\n",
    "            x.append(im.reshape(im_size * im_size))\n",
    "            x[-1] /= 255\n",
    "            y.append(letter)\n",
    "for letter in range(alphabet_size):\n",
    "    path = \"data/big_alphabet_29x29/class-\" + str(letter) + \".bmp\"\n",
    "    im = misc.imread(path, flatten='grey')\n",
    "    x_test.append(im.reshape(im_size * im_size))\n",
    "    x_test[-1] /= 255\n",
    "    y_test.append(letter)\n",
    "x = np.asarray(x)\n",
    "y = np.asarray(y)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "#ytest = np.arange(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(182, 841) (182,) (78, 841) (78,)\n"
     ]
    }
   ],
   "source": [
    "print x.shape, y.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:122: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:123: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:124: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:125: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 inf\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n",
      "9 0.0\n",
      "10 0.0\n",
      "11 0.0\n",
      "12 0.0\n",
      "13 0.0\n",
      "14 0.0\n",
      "15 0.0\n",
      "16 0.0\n",
      "17 0.0\n",
      "18 0.0526315789474\n",
      "19 0.105263157895\n",
      "20 0.0\n",
      "21 0.0\n",
      "22 0.0526315789474\n",
      "23 0.0526315789474\n",
      "24 0.0526315789474\n",
      "25 0.0526315789474\n",
      "26 0.0\n",
      "27 0.105263157895\n",
      "28 0.157894736842\n",
      "29 0.105263157895\n",
      "30 0.105263157895\n",
      "31 0.105263157895\n",
      "32 0.105263157895\n",
      "33 0.105263157895\n",
      "34 0.105263157895\n",
      "35 0.210526315789\n",
      "36 0.105263157895\n",
      "37 0.157894736842\n",
      "38 0.157894736842\n",
      "39 0.157894736842\n",
      "40 0.157894736842\n",
      "41 0.157894736842\n",
      "42 0.210526315789\n",
      "43 0.105263157895\n",
      "44 0.105263157895\n",
      "45 0.0526315789474\n",
      "46 0.263157894737\n",
      "47 0.157894736842\n",
      "48 0.105263157895\n",
      "49 0.263157894737\n",
      "50 0.315789473684\n",
      "51 0.105263157895\n",
      "52 0.263157894737\n",
      "53 0.157894736842\n",
      "54 0.210526315789\n",
      "55 0.263157894737\n",
      "56 0.157894736842\n",
      "57 0.210526315789\n",
      "58 0.315789473684\n",
      "59 0.263157894737\n",
      "60 0.210526315789\n",
      "61 0.210526315789\n",
      "62 0.263157894737\n",
      "63 0.210526315789\n",
      "64 0.263157894737\n",
      "65 0.263157894737\n",
      "66 0.210526315789\n",
      "67 0.210526315789\n",
      "68 0.210526315789\n",
      "69 0.263157894737\n",
      "70 0.315789473684\n",
      "71 0.210526315789\n",
      "72 0.315789473684\n",
      "73 0.210526315789\n",
      "74 0.263157894737\n",
      "75 0.210526315789\n",
      "76 0.263157894737\n",
      "77 0.263157894737\n",
      "78 0.210526315789\n",
      "79 0.315789473684\n",
      "80 0.315789473684\n",
      "81 0.263157894737\n",
      "82 0.263157894737\n",
      "83 0.315789473684\n",
      "84 0.315789473684\n",
      "85 0.368421052632\n",
      "86 0.368421052632\n",
      "87 0.421052631579\n",
      "88 0.315789473684\n",
      "89 0.473684210526\n",
      "90 0.315789473684\n",
      "91 0.473684210526\n",
      "92 0.421052631579\n",
      "93 0.578947368421\n",
      "94 0.368421052632\n",
      "95 0.526315789474\n",
      "96 0.368421052632\n",
      "97 0.263157894737\n",
      "98 0.473684210526\n",
      "99 0.526315789474\n",
      "100 0.473684210526\n",
      "101 0.473684210526\n",
      "102 0.578947368421\n",
      "103 0.578947368421\n",
      "104 0.631578947368\n",
      "105 0.526315789474\n",
      "106 0.473684210526\n",
      "107 0.421052631579\n",
      "108 0.526315789474\n",
      "109 0.631578947368\n",
      "110 0.631578947368\n",
      "111 0.368421052632\n",
      "112 0.631578947368\n",
      "113 0.736842105263\n",
      "114 0.789473684211\n",
      "115 0.684210526316\n",
      "116 0.631578947368\n",
      "117 0.684210526316\n",
      "118 0.473684210526\n",
      "119 0.684210526316\n",
      "120 0.684210526316\n",
      "121 0.684210526316\n",
      "122 0.736842105263\n",
      "123 0.736842105263\n",
      "124 0.736842105263\n",
      "125 0.684210526316\n",
      "126 0.789473684211\n",
      "127 0.736842105263\n",
      "128 0.736842105263\n",
      "129 0.736842105263\n",
      "130 0.736842105263\n",
      "131 0.684210526316\n",
      "132 0.736842105263\n",
      "133 0.736842105263\n",
      "134 0.736842105263\n",
      "135 0.736842105263\n",
      "136 0.631578947368\n",
      "137 0.736842105263\n",
      "138 0.684210526316\n",
      "139 0.736842105263\n",
      "140 0.684210526316\n",
      "141 0.631578947368\n",
      "142 0.684210526316\n",
      "143 0.684210526316\n",
      "144 0.736842105263\n",
      "145 0.736842105263\n",
      "146 0.736842105263\n",
      "147 0.736842105263\n",
      "148 0.684210526316\n",
      "149 0.736842105263\n",
      "150 0.736842105263\n",
      "151 0.789473684211\n",
      "152 0.736842105263\n",
      "153 0.684210526316\n",
      "154 0.684210526316\n",
      "155 0.684210526316\n",
      "156 0.736842105263\n",
      "157 0.684210526316\n",
      "158 0.684210526316\n",
      "159 0.789473684211\n",
      "160 0.684210526316\n",
      "161 0.736842105263\n",
      "162 0.736842105263\n",
      "163 0.684210526316\n",
      "164 0.789473684211\n",
      "165 0.736842105263\n",
      "166 0.684210526316\n",
      "167 0.736842105263\n",
      "168 0.684210526316\n",
      "169 0.736842105263\n",
      "170 0.736842105263\n",
      "171 0.736842105263\n",
      "172 0.789473684211\n",
      "173 0.736842105263\n",
      "174 0.736842105263\n",
      "175 0.736842105263\n",
      "176 0.736842105263\n",
      "177 0.736842105263\n",
      "178 0.789473684211\n",
      "179 0.789473684211\n",
      "180 0.736842105263\n",
      "181 0.736842105263\n",
      "182 0.736842105263\n",
      "183 0.736842105263\n",
      "184 0.789473684211\n",
      "185 0.736842105263\n",
      "186 0.736842105263\n",
      "187 0.736842105263\n",
      "188 0.736842105263\n",
      "189 0.736842105263\n",
      "190 0.736842105263\n",
      "191 0.736842105263\n",
      "192 0.736842105263\n",
      "193 0.789473684211\n",
      "194 0.736842105263\n",
      "195 0.736842105263\n",
      "196 0.736842105263\n",
      "197 0.736842105263\n",
      "198 0.789473684211\n",
      "199 0.789473684211\n",
      "200 0.736842105263\n",
      "201 0.736842105263\n",
      "202 0.736842105263\n",
      "203 0.736842105263\n",
      "204 0.736842105263\n",
      "205 0.736842105263\n",
      "206 0.736842105263\n",
      "207 0.789473684211\n",
      "208 0.736842105263\n",
      "209 0.736842105263\n",
      "210 0.789473684211\n",
      "211 0.789473684211\n",
      "212 0.789473684211\n",
      "213 0.789473684211\n",
      "214 0.736842105263\n",
      "215 0.789473684211\n",
      "216 0.789473684211\n",
      "217 0.736842105263\n",
      "218 0.789473684211\n",
      "219 0.789473684211\n",
      "220 0.736842105263\n",
      "221 0.736842105263\n",
      "222 0.789473684211\n",
      "223 0.736842105263\n",
      "224 0.789473684211\n",
      "225 0.736842105263\n",
      "226 0.736842105263\n",
      "227 0.789473684211\n",
      "228 0.789473684211\n",
      "229 0.789473684211\n",
      "230 0.736842105263\n",
      "231 0.736842105263\n",
      "232 0.789473684211\n",
      "233 0.736842105263\n",
      "234 0.789473684211\n",
      "235 0.789473684211\n",
      "236 0.789473684211\n",
      "237 0.789473684211\n",
      "238 0.789473684211\n",
      "239 0.789473684211\n",
      "240 0.842105263158\n",
      "241 0.842105263158\n",
      "242 0.842105263158\n",
      "243 0.789473684211\n",
      "244 0.789473684211\n",
      "245 0.789473684211\n",
      "246 0.736842105263\n",
      "247 0.789473684211\n",
      "248 0.842105263158\n",
      "249 0.789473684211\n",
      "250 0.842105263158\n",
      "251 0.736842105263\n",
      "252 0.842105263158\n",
      "253 0.789473684211\n",
      "254 0.842105263158\n",
      "255 0.842105263158\n",
      "256 0.789473684211\n",
      "257 0.842105263158\n",
      "258 0.789473684211\n",
      "259 0.842105263158\n",
      "260 0.842105263158\n",
      "261 0.842105263158\n",
      "262 0.842105263158\n",
      "263 0.842105263158\n",
      "264 0.842105263158\n",
      "265 0.842105263158\n",
      "266 0.842105263158\n",
      "267 0.842105263158\n",
      "268 0.842105263158\n",
      "269 0.842105263158\n",
      "270 0.842105263158\n",
      "271 0.842105263158\n",
      "272 0.842105263158\n",
      "273 0.842105263158\n",
      "274 0.842105263158\n",
      "275 0.842105263158\n",
      "276 0.842105263158\n",
      "277 0.947368421053\n",
      "278 0.789473684211\n",
      "279 0.789473684211\n",
      "280 0.842105263158\n",
      "281 0.842105263158\n",
      "282 0.947368421053\n",
      "283 0.842105263158\n",
      "284 0.842105263158\n",
      "285 0.947368421053\n",
      "286 0.842105263158\n",
      "287 0.789473684211\n",
      "288 0.842105263158\n",
      "289 0.842105263158\n",
      "290 0.947368421053\n",
      "291 0.842105263158\n",
      "292 0.842105263158\n",
      "293 0.842105263158\n",
      "294 0.842105263158\n",
      "295 0.947368421053\n",
      "296 0.947368421053\n",
      "297 0.947368421053\n",
      "298 0.947368421053\n",
      "299 0.842105263158\n",
      "[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22, 22, 23, 23, 24, 24, 25, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "nt = network(layers_list=[25, alphabet_size], learning_rate=0.005, alpha=1.0, \\\n",
    "             activation_functions=['sigmoid', 'sigmoid'], cost_func=\"logistic\", \\\n",
    "             mode='class', cou_iter=300, early_stop=-1, regularization = 'l2', reg_param = 0.01, batch_size = 30)\n",
    "nt.fit(x, y)\n",
    "\n",
    "ypred = nt.predict(x_test)\n",
    "print ypred\n",
    "print accuracy_score(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9 10 10 11 11 12\n",
      " 12 13 13 14 14 15 15 16 16 17 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24\n",
      " 25 25  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22\n",
      " 23 24 25]\n"
     ]
    }
   ],
   "source": [
    "print y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
