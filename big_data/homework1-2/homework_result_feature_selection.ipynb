{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CART:    \n",
    "    def __init__(self, features_rate, min_samples_leaf, max_depth):\n",
    "        self.features_rate = features_rate\n",
    "        self.tree_map = {}\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        if (max_depth == None):\n",
    "            self.max_depth = 100000\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "\n",
    "    def gen_tree(self, element_numbers, vertex_num, mean, now_depth, mse_value):\n",
    "         \n",
    "        if (now_depth == self.max_depth):  \n",
    "            self.tree_map[str(vertex_num)] = [[None, None], [1], [mean], [None, None]]\n",
    "            return 0\n",
    "        \n",
    "        min_mse = mse_value\n",
    "\n",
    "        pos_num_split = np.nan\n",
    "        value_split = np.nan\n",
    "        \n",
    "        left = []\n",
    "        right = []\n",
    "        \n",
    "        cou_elements = len(element_numbers)\n",
    "        \n",
    "        best_left_square_sum = np.nan\n",
    "        best_left_sum = np.nan\n",
    "        best_right_square_sum = np.nan\n",
    "        best_right_sum = np.nan\n",
    "        \n",
    "        x = self.x[element_numbers]\n",
    "        y = self.y[element_numbers]\n",
    "\n",
    "        for feature_num in range(self.features_cou_analize):\n",
    "\n",
    "            sort_ind = x[:,feature_num].argsort()\n",
    "            new_x = x[:,feature_num][sort_ind]\n",
    "            new_y = y[sort_ind]\n",
    "            \n",
    "            left_cou = 0\n",
    "            right_cou = cou_elements\n",
    "            \n",
    "            del_num = 0\n",
    "            right_square_sum = mse_value + mean * mean * cou_elements * 1.0\n",
    "            right_sum = mean * cou_elements * 1.0\n",
    "            left_square_sum = 0.0\n",
    "            left_sum = 0.0\n",
    "            feature_value_split = np.nan\n",
    "            for i, feature_value in enumerate(new_x):\n",
    "                \n",
    "                if (feature_value_split == feature_value):\n",
    "                    continue\n",
    "                    \n",
    "                if (np.isnan(feature_value_split)):\n",
    "                    feature_value_split = feature_value\n",
    "                else:\n",
    "                    feature_value_split = 0.5 * (feature_value_split + feature_value)\n",
    "                    \n",
    "                while (1):\n",
    "                    if (new_x[del_num] < feature_value_split):\n",
    "                        y_val = new_y[del_num]\n",
    "                        del_num += 1\n",
    "                        left_cou += 1\n",
    "                        right_cou -= 1\n",
    "                        \n",
    "                        left_square_sum += y_val * y_val\n",
    "                        left_sum += y_val\n",
    "                        \n",
    "                        right_square_sum -= y_val * y_val\n",
    "                        right_sum -= y_val\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if (left_cou < self.min_samples_leaf):\n",
    "                    continue\n",
    "                elif (right_cou < self.min_samples_leaf):\n",
    "                    break\n",
    "                \n",
    "                now_left_mse = left_square_sum - left_sum * left_sum * 1.0/ left_cou\n",
    "                now_right_mse = right_square_sum - right_sum * right_sum * 1.0 / right_cou\n",
    "                \n",
    "                if (now_left_mse + now_right_mse < min_mse):\n",
    "                    min_mse = now_left_mse + now_right_mse\n",
    "                    pos_num_split = feature_num\n",
    "                    value_split = feature_value_split\n",
    "                    \n",
    "                    best_left_square_sum = left_square_sum\n",
    "                    best_left_sum = left_sum\n",
    "                    \n",
    "                    best_right_square_sum = right_square_sum\n",
    "                    best_right_sum = right_sum\n",
    "                    \n",
    "        if (np.isnan(value_split)):\n",
    "            self.tree_map[str(vertex_num)]= [[None, None], [1], [mean], [None, None]]\n",
    "            return 0\n",
    "        \n",
    "        self.tree_map[str(vertex_num)] = [[self.feature_order[pos_num_split], value_split], [0], [mean], [vertex_num*2 + 1, vertex_num*2 + 2]]\n",
    "        \n",
    "        \n",
    "        left_class = []\n",
    "        right_class = []\n",
    "        \n",
    "        for pos in element_numbers:\n",
    "            if (self.x[pos][pos_num_split] < value_split):\n",
    "                left.append(pos)\n",
    "            else:\n",
    "                right.append(pos)\n",
    "\n",
    "        left_mse = best_left_square_sum - (best_left_sum ** 2.0) / len(left)\n",
    "        right_mse = best_right_square_sum - (best_right_sum ** 2.0) / len(right)\n",
    "        \n",
    "        self.feature_contribution[self.feature_order[pos_num_split]] += (mse_value - left_mse - right_mse)\n",
    "        \n",
    "        self.gen_tree(left, vertex_num*2 + 1, best_left_sum * 1.0 / len(left), now_depth + 1, left_mse)\n",
    "        self.gen_tree(right, vertex_num*2 + 2, best_right_sum *1.0/ len(right), now_depth + 1, right_mse)\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.tree_map = {}\n",
    "        \n",
    "        x = np.asarray(x, dtype = np.float)\n",
    "        features_cou = (x.shape)[1]\n",
    "        self.feature_order = np.sort(np.random.choice((x.shape)[1], int((x.shape)[1] * (self.features_rate)), replace=False))\n",
    "        \n",
    "        self.features_cou_analize = int((x.shape)[1] * (self.features_rate))\n",
    "        self.cou_samples = len(x)\n",
    "        \n",
    "        self.feature_contribution = np.zeros(features_cou)\n",
    "        \n",
    "        for i in reversed(range(features_cou)):\n",
    "            if (i not in self.feature_order):\n",
    "                x = np.delete(x, i, 1)\n",
    "\n",
    "        self.x = x\n",
    "        self.y = np.asarray(y, dtype = np.float)\n",
    "        y_pred = np.tile(np.sum(y) * 1.0 / len(y), len(y))\n",
    "        \n",
    "        self.gen_tree(np.arange(self.cou_samples), 0, np.sum(y) * 1.0 / len(y), 0, np.sum((y_pred - self.y) ** 2.0))\n",
    "        \n",
    "        return self.tree_map\n",
    "\n",
    "    def predict(self, data):\n",
    "        y_pred = []\n",
    "        for sample in data:\n",
    "            now_vertex = 0\n",
    "            while (1):\n",
    "                if (self.tree_map[str(now_vertex)][1][0] == 1):\n",
    "                    break\n",
    "                if (sample[self.tree_map[str(now_vertex)][0][0]] >= self.tree_map[str(now_vertex)][0][1]):\n",
    "                    now_vertex = self.tree_map[str(now_vertex)][3][1]\n",
    "                else:\n",
    "                    now_vertex = self.tree_map[str(now_vertex)][3][0]\n",
    "            y_pred.append(self.tree_map[str(now_vertex)][2][0])\n",
    "        return y_pred\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"spam.train.txt\", \"r\")\n",
    "y_true = []\n",
    "data = []\n",
    "for line in f:\n",
    "    line = line[:len(line) - 1]\n",
    "    arr = line.split(' ')\n",
    "    y_true.append(arr[0])\n",
    "    del arr[0]\n",
    "    data.append(arr)\n",
    "x = np.asarray(data, dtype = np.float)\n",
    "y = np.asarray(y_true, dtype = np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"spam.test.txt\", \"r\")\n",
    "test_data = []\n",
    "test_y = []\n",
    "for line in f:\n",
    "    line = line[:len(line) - 1]\n",
    "    arr = line.split(' ')\n",
    "    test_y.append(arr[0])\n",
    "    del arr[0]\n",
    "    test_data.append(arr)\n",
    "    \n",
    "test_x = np.asarray(test_data, dtype = np.float)\n",
    "test_y = np.asarray(test_y, dtype = np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class modify_gb:\n",
    "    \n",
    "    def __init__(self, n_estimators, features_rate, cou_samples, min_samples_split, learning_rate, max_depth, loss, regularizator_value):\n",
    "        self.regularizator_value = regularizator_value\n",
    "        self.number_elements_in_subsample = cou_samples / n_estimators\n",
    "        self.n_estimators = n_estimators\n",
    "        self.features_rate = features_rate\n",
    "        self.cou_samples = cou_samples\n",
    "        self.loss = loss\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.shrinkage_value = learning_rate\n",
    "        if (max_depth == None):\n",
    "            self.max_depth = 10000\n",
    "        else:\n",
    "            self.max_depth = max_depth\n",
    "    \n",
    "    def get_predict_for_sample_and_one_tree(self, sample, tree_map):\n",
    "        now_vertex = 0\n",
    "        while (1):\n",
    "            if (tree_map[str(now_vertex)][1][0] == 1):\n",
    "                break\n",
    "            if (sample[tree_map[str(now_vertex)][0][0]] >= tree_map[str(now_vertex)][0][1]):\n",
    "                now_vertex = tree_map[str(now_vertex)][3][1]\n",
    "            else:\n",
    "                now_vertex = tree_map[str(now_vertex)][3][0]\n",
    "        return tree_map[str(now_vertex)][2][0]\n",
    "    \n",
    "    def get_predict_for_sample(self, sample):\n",
    "        answer = 0.0\n",
    "        for i in range(len(self.trees_list)):\n",
    "            answer += self.shrinkage_value * self.get_predict_for_sample_and_one_tree(sample, self.trees_list[i][1])\n",
    "        return answer\n",
    "    \n",
    "    def update_grad_for_l2(self, x, tree_map):\n",
    "        for i, sample in enumerate(x):\n",
    "            self.now_grad[i] -= 2.0 * self.get_predict_for_sample_and_one_tree(sample, tree_map) * self.shrinkage_value\n",
    "            \n",
    "    def update_grad_for_l1(self, x, tree_map):\n",
    "        for i, sample in enumerate(x):\n",
    "            pred_for_sample = self.get_predict_for_sample_and_one_tree(sample, tree_map) * self.shrinkage_value\n",
    "            self.y_balance[i] -= pred_for_sample\n",
    "            if (self.y_balance[i] == 0.0):\n",
    "                self.now_grad[i] = 0.00000001\n",
    "            elif (self.y_balance[i] > 0.0):\n",
    "                self.now_grad[i] = 1.0\n",
    "            elif (self.y_balance[i] < 0.0):\n",
    "                self.now_grad[i] = -1.0\n",
    "        \n",
    "        \n",
    "    def predict_regression(self, x):\n",
    "        y_pred = np.zeros(len(x))\n",
    "        for i, sample in enumerate(x):\n",
    "            y_pred[i] = self.get_predict_for_sample(sample)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict_regression_after_select(self, x):\n",
    "        for i in reversed(xrange(x.shape[1])):\n",
    "            if (i not in self.good_features):\n",
    "                x = np.delete(x, i, axis = 1)\n",
    "                \n",
    "        y_pred = np.zeros(len(x))\n",
    "        for i, sample in enumerate(x):\n",
    "            y_pred[i] = self.get_predict_for_sample(sample)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.pos_arr = np.arange(self.cou_samples)\n",
    "        self.now_grad = np.array(y, dtype = np.float)\n",
    "        self.trees_list = []\n",
    "        self.y_balance = np.array(y, dtype = np.float)\n",
    "        \n",
    "        cart = CART(self.features_rate, self.min_samples_split, self.max_depth)\n",
    "        \n",
    "        self.feature_contribution = np.zeros((x.shape)[1])\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            np.random.shuffle(self.pos_arr)\n",
    "            index = self.pos_arr[:self.cou_samples / 2]\n",
    "            now_x = x[index]\n",
    "            now_y = self.now_grad[index]\n",
    "            \n",
    "            cart.fit(now_x, now_y)\n",
    "            if (i == 0 and self.loss == 'l2'):\n",
    "                self.now_grad *= 2.0\n",
    "            elif (i == 0 and self.loss == 'l1'):\n",
    "                self.now_grad = np.zeros(len(self.now_grad))\n",
    "                \n",
    "            self.feature_contribution += cart.feature_contribution\n",
    "            \n",
    "            self.trees_list.append([self.shrinkage_value, cart.tree_map])\n",
    "            \n",
    "            if (self.loss == 'l2'):\n",
    "                self.update_grad_for_l2(x, cart.tree_map)\n",
    "            elif (self.loss == 'l1'):\n",
    "                self.update_grad_for_l1(x, cart.tree_map)\n",
    "                \n",
    "        return self.trees_list\n",
    "    \n",
    "    def feature_selection_and_fit(self, x, y):\n",
    "        self.fit(x, y)\n",
    "        feature_probability = self.feature_contribution / np.sum(self.feature_contribution)\n",
    "        self.good_features = []\n",
    "        for i in reversed(xrange(x.shape[1])):\n",
    "            if (feature_probability[i] < self.regularizator_value):\n",
    "                x = np.delete(x, i, axis = 1)\n",
    "            else:\n",
    "                self.good_features.append(i)\n",
    "                \n",
    "        self.fit(x, y)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add noise features and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature1 = np.random.binomial(10000, 0.9, len(y_true))\n",
    "feature2 = np.random.binomial(1000, 0.8, len(y_true))\n",
    "feature3 = np.random.normal(0, 10, len(y_true))\n",
    "feature4 = np.random.normal(21, 15, len(y_true))\n",
    "feature5 = np.random.geometric(p=0.1, size=len(y_true))\n",
    "feature6 = np.random.chisquare(15,len(y_true))\n",
    "feature7 = np.random.uniform(-10,10,len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.insert(data, 0, feature7, axis = 1)\n",
    "data = np.insert(data, 0, feature6, axis = 1)\n",
    "data = np.insert(data, 0, feature5, axis = 1)\n",
    "data = np.insert(data, 0, feature4, axis = 1)\n",
    "data = np.insert(data, 0, feature3, axis = 1)\n",
    "data = np.insert(data, 0, feature2, axis = 1)\n",
    "data = np.insert(data, 0, feature1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature1 = np.random.binomial(10000, 0.9, len(test_y))\n",
    "feature2 = np.random.binomial(1000, 0.8, len(test_y))\n",
    "feature3 = np.random.normal(0, 10, len(test_y))\n",
    "feature4 = np.random.normal(21, 15, len(test_y))\n",
    "feature5 = np.random.geometric(p=0.1, size=len(test_y))\n",
    "feature6 = np.random.chisquare(15,len(test_y))\n",
    "feature7 = np.random.uniform(-10,10,len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = np.insert(test_data, 0, feature7, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature6, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature5, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature4, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature3, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature2, axis = 1)\n",
    "test_data = np.insert(test_data, 0, feature1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 80, 'features_rate': 1.0, 'cou_samples': len(data), 'min_samples_split': 10,\n",
    "                    'learning_rate' : 0.03, 'max_depth' : 4, 'loss' : 'l2', 'regularizator_value' : 0.007}\n",
    "clf_without_select = modify_gb(**params)\n",
    "clf_with_select = modify_gb(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_with_select.feature_selection_and_fit(x, y)\n",
    "y_pred = clf_with_select.predict_regression_after_select(test_x)\n",
    "print sk.metrics.mean_squared_error(y_pred, test_y)\n",
    "print clf_with_select.feature_contribution\n",
    "print clf_with_select.good_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print clf_with_select.feature_contribution / np.sum(clf_with_select.feature_contribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_without_select.fit(x, y)\n",
    "y_pred = clf_without_select.predict_regression(test_x)\n",
    "print sk.metrics.mean_squared_error(y_pred, test_y)\n",
    "print clf_without_select.feature_contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = test_y.copy()\n",
    "y1 = test_y.copy()\n",
    "estimators_list = np.linspace(1, 100, 100)\n",
    "mse_select = []\n",
    "mse_without_select = []\n",
    "cou_samples = len(test_y)\n",
    "y = np.array(y, dtype = np.float)\n",
    "y1 = np.array(y1, dtype = np.float)\n",
    "for i in range(0, 100):\n",
    "    for j, sample in enumerate(test_data):\n",
    "        y[j] -= clf_with_select.get_predict_for_sample_and_one_tree(sample, clf_with_select.trees_list[i][1]) * clf_with_select.shrinkage_value\n",
    "        y1[j] -= clf_without_select.get_predict_for_sample_and_one_tree(sample, clf_without_select.trees_list[i][1]) * clf_without_select.shrinkage_value\n",
    "    if (i + 1 in estimators_list):\n",
    "        mse_select.append(np.sum(y ** 2.0) * 1.0 / cou_samples)\n",
    "        mse_without_select.append(np.sum(y1 ** 2.0) * 1.0 / cou_samples)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "estimators_list = np.asarray(estimators_list, dtype = np.float)\n",
    "plt.figure(figsize = (8, 6), dpi = 80)\n",
    "plt.xlabel('cou_estimators')\n",
    "plt.ylabel('sklearn regression boosting error')\n",
    "select, = plt.plot(estimators_list, mse_select, c = 'g', label = \"select line\")\n",
    "without_select, = plt.plot(estimators_list, mse_without_select, c = 'y', label = \"without_select\")\n",
    "plt.legend(handles = [select, without_select])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
